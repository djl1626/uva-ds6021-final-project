---
title: "DS6021 Final Project"
author: "Daniel Luettgen"
date: "8/5/2024"
output: html_document
---

## Cleaning the Data

```{python eval=FALSE, include=FALSE, python.reticulate=FALSE}
import pandas as pd

stars = pd.read_csv('Stars.csv')

replace_map = {"Red": "Red_Orange",
"Blue": "Blue",
"Blue-white": "Blue-White",
"Blue White": "Blue-White",
"yellow-white": "Yellow_White",
"White": "Yellow_White",
"Blue white": "Blue-White",
"white": "Yellow_White",
"Yellowish White": "Yellow_White",
"yellowish": "Yellow_White",
"Whitish": "Yellow_White",
"Orange": "Red_Orange",
"White-Yellow": "Yellow_White",
"Pale yellow orange": "Yellow_White",
"Yellowish": "Yellow_White",
"Orange-Red": "Red_Orange",
"Blue-White": "Blue-White"}

stars.Color = stars.Color.replace(replace_map)

replace_map2 = {,
   0: 'Red Dwarf'
   1: 'Brown Dwarf',
   2: 'White Dwarf',
   3: 'Main Sequence',
   4: 'Super Giants',
   5" 'Hyper Giants'
}

stars.Type = stars.Type.replace(replace_map2)

stars.to_csv('Stars_clean_r.csv', index = False)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggcorrplot)
library(MASS)
library(caret)
library(car)
library(broom)
library(glmnet)
library(gridExtra)
library(nnet)

stars <- read.csv("Data/Stars_cleanest_r.csv")
```

## Data Exploration

```{r numerical-distribution}
knitr::kable(summary(stars %>% dplyr::select(Temperature, L, R, A_M)), format='html')
```

The table above shows the distribution of numerical data in our dataset. Temperature, Luminosity, and Radius all have a very wide distribution, signaling that scaling may be necessary when constructing linear models.

## Multiple Linear Regression - LASSO Approach

```{r initial-model}
inimodel <- lm(Temperature~., data=stars[0:5])
summary(inimodel)
```

This naive initial model predicts Temperature from Luminosity, Radius, Absolute Magnitude, and Color. While the overall model is significant (p-value ~0), all of the numerical predictors are not significant. Referring back to the exploratory analysis section, this may be because of the large spread in data for Luminosity and Radius.

```{r initial-log-model}
stars2 <- mutate(stars, Temperature = log(Temperature), L = log(L), R = log(R))
inimodel2 <- lm(Temperature~., data=stars2[0:5])
summary(inimodel2)
```

After transforming the Luminosity, Radius, and Absolute Magnitude values, the Adj. $R^2$ for the model increased by almost .2. All of the colors remained significant. The p-values for the numerical variables are still high, and we have known multicollinearity issues with Absolute Magnitude and Radius. 

To perform feature selection, we turn to LASSO regularization.

```{r lasso-model-plot}
X<- model.matrix(Temperature~0+.,data=stars2[0:5])
y <- stars2[0:5]$Temperature
rmodel <- glmnet(x=X, y=y, alpha = 1)
kcvrmodel <- cv.glmnet(x=X, y=y, alpha = 1, nfolds=10)
# kcvrmodel$lambda.1se
plot(rmodel, label=T,xvar='lambda') + abline(v=log(kcvrmodel$lambda.1se))
```

```{r lasso-coefs}
predict(rmodel, type="coefficient", s=kcvrmodel$lambda.1se, newx=X[1:10,])
```

Using a lambda value 1 standard deviation away from the lambda that minimizes RMSE, we see the LASSO regression has eliminated the Radius and Absolute Magnitude predictors. Seeing at least one of these removed was expected because these two predictors were highly correlated after transforming Radius. Although the LASSO regression eliminated the Yellow-White color, we opted to leave this categorical predictor in the model since the other 3 colors were found to be significant.

```{r good-model}
goodmodel <- lm(Temperature~L+Color, data=stars2)
summary(goodmodel)
```

Following feature selection using LASSO regression, we created our final linear model using the chosen features. All of the predictors are shown to be significant in the model with p-values < .01. 

The Adj. $R^2$ and RMSE are about the same as the model pre-feature selection, but based on the principal of parsimony, we conclude that this model is better.

```{r jitter-plot}
ggplot(stars2, aes(x=L, y = Temperature, color=Color))+geom_jitter()+
  geom_smooth(method="lm", model.extract(stars2), se=FALSE)
```

#### Checking Assumptions

```{r good-model-resid-plot}
star_pred <- mutate(stars2, predictions=fitted(goodmodel), resid=residuals(goodmodel))

ggplot(star_pred, aes(x=predictions, y=resid)) + geom_point() + geom_hline(yintercept = 0, color="red")
```

Generally speaking, the residuals plot is evenly distributed around the $y=0$ line. There is some vertical patterning, but no distinct fanning in the residuals. The vertical patterning is of some concern, but this mostly corresponds to the colors of stars in the data-set.

```{r good-model-qq-plot}
ggplot(star_pred, aes(sample=resid)) + stat_qq() + stat_qq_line(color="red")
```

The QQ plot displays excellent results within the $[-1, 1]$ quantile range. However, after that we see the points diverge heavily from the $y=x$ line. This is an indication that the population may not be normally distributed in the extreme ranges of the data. This divergence calls the 4th assumption of a normal population into question.

#### Conculusion

Although we have a model that has relatively strong performance metrics, we struggle to validate all of the assumptions necessary for linear modeling. In the end, a linear model is likely not the best choice for our data-set. 

```{r}
new_dat <- stars2[c(90,150,175,220),-1]
exp(predict(goodmodel, newdata = new_dat, interval = "prediction", level = .95))
exp(predict(goodmodel, newdata = new_dat, interval = "confidence", level = .95))
exp(stars2[c(90,150,175,220),1:2])
```

## Logistic Regression

### One Vs. All

One Vs. All logistic regression is a method to use logistic regression to predict multiclass categorical variables. We will be attempting to predict star type from the natural log of Temperature, Luminosity, and Radius. For each star type, we train a logistic regression model that predicts either that star type, or not that star type. For example, the first model will predict whether a star is a Brown Dwarf or not a Brown Dwarf. At the end, the star type with the largest predicted probability will be labeled as the prediction from the model.

```{r read-dataset}
# set a seed
set.seed(6021)

# Read in the data-set. Drop the first column
stars <- read.csv('Data/Stars_cleanest_r.csv')
stars <- stars %>% dplyr::select(Temperature, L, R, A_M, Color, Spectral_Class, Type)
stars <- stars %>% mutate(log_L = log(L), log_R=log(R), log_Temp=log(Temperature))
dummies <- dummyVars('~Type', data=stars)
stars_dummies <- data.frame(stars, predict(dummies, newdata=stars))
```

```{r train-test-split}
# Split the data into train and test
train_sample <- sample(1:nrow(stars_dummies), .8*nrow(stars_dummies), replace=FALSE)
train_data <- stars_dummies[train_sample,]
test_data <- stars_dummies[-train_sample,]
```

We broke the data-set into a train and test data-set to evaluate model performance after training the model. A ridge regularization penalty was added to the model to introduce bias into the predictions. This is because some of the models without the penalty displayed perfect separation in the data, which yielded excellent accuracy, but poor p-values for the predictors.

```{r}
# Brown Dwarf Model
bd_X <- model.matrix(TypeBrown.Dwarf~0+log_L+log_R+log_Temp, data=train_data)
bd_y <- train_data$TypeBrown.Dwarf

bd_ridge_model <- glmnet(x=bd_X, y=bd_y, data=stars, alpha=0, family='binomial')

bd_cv_model <- cv.glmnet(x=bd_X, y=bd_y, data=stars, alpha=0, family='binomial', nfolds=10)

bd_train_probs <- predict(bd_ridge_model, newx=bd_X, s=bd_cv_model$lambda.min, type='response')
```

```{r}
# Red Dwarf Model
rd_X <- model.matrix(TypeRed.Dwarf~0+log_L+log_R+log_Temp, data=train_data)
rd_y <- train_data$TypeRed.Dwarf

rd_ridge_model <- glmnet(x=rd_X, y=rd_y, data=stars, alpha=0, family='binomial')

rd_cv_model <- cv.glmnet(x=rd_X, y=rd_y, data=stars, alpha=0, family='binomial', nfolds=10)

rd_train_probs <- predict(rd_ridge_model, newx=rd_X, s=rd_cv_model$lambda.min, type='response')
```

```{r}
# White Dwarf Model
wd_X <- model.matrix(TypeWhite.Dwarf~0+log_L+log_R+log_Temp, data=train_data)
wd_y <- train_data$TypeWhite.Dwarf

wd_ridge_model <- glmnet(x=wd_X, y=wd_y, data=stars, alpha=0, family='binomial')

wd_cv_model <- cv.glmnet(x=wd_X, y=wd_y, data=stars, alpha=0, family='binomial', nfolds=10)

wd_train_probs <- predict(wd_ridge_model, newx=wd_X, s=wd_cv_model$lambda.min, type='response')
```

```{r}
# Main Sequence Model
ms_X <- model.matrix(TypeMain.Sequence~0+log_L+log_R+log_Temp, data=train_data)
ms_y <- train_data$TypeMain.Sequence

ms_ridge_model <- glmnet(x=ms_X, y=ms_y, data=stars, alpha=0, family='binomial')

ms_cv_model <- cv.glmnet(x=ms_X, y=ms_y, data=stars, alpha=0, family='binomial', nfolds=10)

ms_train_probs <- predict(ms_ridge_model, newx=ms_X, s=ms_cv_model$lambda.min, type='response')
```

```{r}
# Hyper Giants Model
hg_X <- model.matrix(TypeHyper.Giants~0+log_L+log_R+log_Temp, data=train_data)
hg_y <- train_data$TypeHyper.Giants

hg_ridge_model <- glmnet(x=hg_X, y=hg_y, data=stars, alpha=0, family='binomial')

hg_cv_model <- cv.glmnet(x=hg_X, y=hg_y, data=stars, alpha=0, family='binomial', nfolds=10)

hg_train_probs <- predict(hg_ridge_model, newx=hg_X, s=hg_cv_model$lambda.min, type='response')
```

```{r}
# Super Giants Model
sg_X <- model.matrix(TypeSuper.Giants~0+log_L+log_R+log_Temp, data=train_data)
sg_y <- train_data$TypeSuper.Giants

sg_ridge_model <- glmnet(x=sg_X, y=sg_y, data=stars, alpha=0, family='binomial')

sg_cv_model <- cv.glmnet(x=sg_X, y=sg_y, data=stars, alpha=0, family='binomial', nfolds=10)

sg_train_probs <- predict(sg_ridge_model, newx=sg_X, s=sg_cv_model$lambda.min, type='response')
```

```{r evaluate-train}
train_probs <- data.frame(bd_train_probs, rd_train_probs, wd_train_probs, ms_train_probs, hg_train_probs, sg_train_probs)
colnames(train_probs) <- c('Brown Dwarf', 'Red Dwarf', 'White Dwarf', 'Main Sequence', 'Hyper Giants', 'Super Giants')

train_rowmax <- apply(train_probs, 1, max)
train_log_odds <- train_rowmax / log(1-train_rowmax)
train_rowmax_index <- apply(train_probs, 1, which.max)

train_preds <- colnames(train_probs)[train_rowmax_index]

paste('Training Accuracy:', sum((as.integer(train_preds == train_data$Type)) / nrow(train_data)))

M_train <- as.matrix(train_data[,11:16])
colnames(M_train) <- c("Brown Dwarf", "Hyper Giants", "Main Sequence", "Red Dwarf", "Super Giants", "White Dwarf")
pROC::multiclass.roc(train_preds, M_train)
```

After training each logistic regression model, we evaluate the performance of the combination of models. We predict the star type based on which star type's logistic regression model yielded the largest probability against all others. With the training data, we have ~90% accuracy and ~.94 AUC, indicating an effective model against the training data.

We then turn to testing the trained model against the testing data-set.

```{r test-model}
# Test the model
bd_test_X <- model.matrix(TypeBrown.Dwarf~0+log_L+log_R+log_Temp, data=test_data)
bd_test_y <- test_data$TypeBrown.Dwarf
bd_test_probs <- predict(bd_ridge_model, newx=bd_test_X, s=bd_cv_model$lambda.min, type='response')

rd_test_X <- model.matrix(TypeRed.Dwarf~0+log_L+log_R+log_Temp, data=test_data)
rd_test_y <- test_data$TypeRed.Dwarf
rd_test_probs <- predict(rd_ridge_model, newx=rd_test_X, s=rd_cv_model$lambda.min, type='response')

wd_test_X <- model.matrix(TypeWhite.Dwarf~0+log_L+log_R+log_Temp, data=test_data)
wd_test_y <- test_data$TypeWhite.Dwarf
wd_test_probs <- predict(wd_ridge_model, newx=wd_test_X, s=wd_cv_model$lambda.min, type='response')

ms_test_X <- model.matrix(TypeMain.Sequence~0+log_L+log_R+log_Temp, data=test_data)
ms_test_y <- test_data$TypeMain.Sequence
ms_test_probs <- predict(ms_ridge_model, newx=ms_test_X, s=ms_cv_model$lambda.min, type='response')

hg_test_X <- model.matrix(TypeHyper.Giants~0+log_L+log_R+log_Temp, data=test_data)
hg_test_y <- test_data$TypeHyper.Giants
hg_test_probs <- predict(hg_ridge_model, newx=hg_test_X, s=hg_cv_model$lambda.min, type='response')

sg_test_X <- model.matrix(TypeSuper.Giants~0+log_L+log_R+log_Temp, data=test_data)
sg_test_y <- test_data$TypeSuper.Giants
sg_test_probs <- predict(sg_ridge_model, newx=sg_test_X, s=sg_cv_model$lambda.min, type='response')

test_probs <- data.frame(bd_test_probs, rd_test_probs, wd_test_probs, ms_test_probs, hg_test_probs, sg_test_probs)
colnames(test_probs) <- c('Brown Dwarf', 'Red Dwarf', 'White Dwarf', 'Main Sequence', 'Hyper Giants', 'Super Giants')

test_rowmax <- apply(test_probs, 1, max)
test_log_odds <- test_rowmax / log(1-test_rowmax)
test_rowmax_index <- apply(test_probs, 1, which.max)

test_preds <- colnames(test_probs)[test_rowmax_index]

paste('testing Accuracy:', sum((as.integer(test_preds == test_data$Type)) / nrow(test_data)))

M_test <- as.matrix(test_data[,11:16])
colnames(M_test) <- c("Brown Dwarf", "Hyper Giants", "Main Sequence", "Red Dwarf", "Super Giants", "White Dwarf")
pROC::multiclass.roc(test_preds, M_test)
```

The model performs slightly worse on the testing data-set, but this is generally expected. We have an accuracy of 77% and an AUC of ~.87. The model overall performs well against the test data.

Next, we turn to evaluating the linearity assumption for logistic regression.

```{r assumptions-plot}
log_L_plot <- ggplot(mapping=aes(x=train_data$log_L, y=train_log_odds)) + geom_point() + xlab('log(L)')
log_R_plot <- ggplot(mapping=aes(x=train_data$log_R, y=train_log_odds)) + geom_point() + xlab('log(R)')
log_Temp_plot <- ggplot(mapping=aes(x=train_data$log_Temp, y=train_log_odds)) + geom_point() + xlab('log(Temperature)')
grid.arrange(log_L_plot, log_R_plot, log_Temp_plot)
```

The plots above show each predictor against the predicted log odds of the train data-set. None of the plots show a distinct linear relationship as they all show some parabolic curving. This brings the linearity assumption into question for this One Vs. All logistic model. Although the model performs well, it may not perform well on future data. In order to dive deeper into validating the linearity assumption, or improve model performance, a larger data-set would be useful. 

### Multinomial Regression

In addition to one-vs-all logistic regression, we also found a second method for predicting a categorical variable with more than 2 features known as multinomial logistic regression. The difference between multinomial logistic regression and one-vs-all logistic regression is that multinomial minimizes the negative log-odds that of each class relative to a reference class rather than all of the other classes.

```{r}
stars <- read.csv("/Users/lathangregg/Documents/MSDS/DS6021/Project/uva-ds6021-final-project/Data/Stars_clean_r.csv")

# Convert 'Type' column to factor
stars$Type <- as.factor(stars$Type)

# Remove rows with missing values
stars <- na.omit(stars)

# Remove categorical columns except 'Type'
stars_numeric <- stars[, sapply(stars, is.numeric)]
stars_numeric$Type <- stars$Type

# Create a training and testing set
set.seed(1)
trainIndex <- createDataPartition(stars_numeric$Type, p = 0.8, list = FALSE)
trainData <- stars_numeric[trainIndex, ]
testData <- stars_numeric[-trainIndex, ]

# Separate predictors and response
x_train <- trainData[, -which(names(trainData) == "Type")]
y_train <- trainData$Type
x_test <- testData[, -which(names(testData) == "Type")]
y_test <- testData$Type

# Train the multinomial logistic regression model
mlr_model <- multinom(Type ~ ., data = trainData, model = T)

summary(mlr_model)

```

The above code cleans the data, creates an 80/20 train/test split and trains the model with the training data. The summary of the model is generated using the summary() function. The summary includes the final coefficients for each model. The coefficients show the log-odds of each class compared to the reference class. To make predictions on test data, the class with the greatest odds for the test data is returned as the prediction. If all of the odds are less than one, the reference class is predicted.

Next, we test our model on our testing data.

```{r}
# Predict on the test set
mlr_pred <- predict(mlr_model, newdata = testData)

# Evaluate the model
conf_matrix <- confusionMatrix(mlr_pred, y_test)
accuracy <- conf_matrix$overall['Accuracy']
cat("Accuracy of Multinomial Logistic Regression: ", accuracy, "\n")
print(conf_matrix)
```
 
The model is able to accurately identify the correct class of the test data 98% of the time. While these predictions are very accurate, it should be noted that the linearity assumption for multinomial logistic regression is not met.

