---
title: "DS6021 Final Project"
author: "Daniel Luettgen"
date: "8/5/2024"
output: html_document
---

```{python}
import pandas as pd

stars = pd.read_csv('Stars.csv')

replace_map = {"Red": "Red_Orange",
"Blue": "Blue",
"Blue-white": "Blue-White",
"Blue White": "Blue-White",
"yellow-white": "Yellow_White",
"White": "Yellow_White",
"Blue white": "Blue-White",
"white": "Yellow_White",
"Yellowish White": "Yellow_White",
"yellowish": "Yellow_White",
"Whitish": "Yellow_White",
"Orange": "Red_Orange",
"White-Yellow": "Yellow_White",
"Pale yellow orange": "Yellow_White",
"Yellowish": "Yellow_White",
"Orange-Red": "Red_Orange",
"Blue-White": "Blue-White"}

stars.Color = stars.Color.replace(replace_map)

stars.to_csv('Stars_clean_r.csv', index = False)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggcorrplot)
library(MASS)
library(caret)
library(car)
library(broom)
library(glmnet)

stars <- read.csv("Data/Stars_clean_r.csv")
```

## Data Exploration

```{r numerical-distribution}
knitr::kable(summary(stars %>% dplyr::select(Temperature, L, R, A_M)), format='html')
```

The table above shows the distribution of numerical data in our dataset. Temperature, Luminosity, and Radius all have a very wide distribution, signaling that scaling may be necessary when constructing linear models.

## Multiple Linear Regression - LASSO Approach

```{r initial-model}
inimodel <- lm(Temperature~., data=stars[0:5])
summary(inimodel)
```

This naive initial model predicts Temperature from Luminosity, Radius, Absolute Magnitude, and Color. While the overall model is significant (p-value ~0), all of the numerical predictors are not significant. Referring back to the exploratory analysis section, this may be because of the large spread in data for Luminosity and Radius.

```{r initial-log-model}
stars2 <- mutate(stars, Temperature = log(Temperature), L = log(L), R = log(R))
inimodel2 <- lm(Temperature~., data=stars2[0:5])
summary(inimodel2)
```

After transforming the Luminosity, Radius, and Absolute Magnitude values, the Adj. $R^2$ for the model increased by almost .2. All of the colors remained significant. The p-values for the numerical variables are still high, and we have known multicollinearity issues with Absolute Magnitude and Radius. 

```{r find-rmse}
predictions <- exp(predict(inimodel2, stars2))
print(paste('RMSE:',sqrt(mean((predictions - stars$Temperature)^2))))
```

Since we transformed the response variable using the natural log function, we need to un-transform the predictions to get an accurate RMSE.

To perform feature selection, we turn to LASSO regularization.

```{r lasso-model-plot}
X<- model.matrix(Temperature~0+.,data=stars2[0:5])
y <- stars2[0:5]$Temperature
rmodel <- glmnet(x=X, y=y, alpha = 1)
kcvrmodel <- cv.glmnet(x=X, y=y, alpha = 1, nfolds=10)
# kcvrmodel$lambda.1se
plot(rmodel, label=T,xvar='lambda') + abline(v=log(kcvrmodel$lambda.1se))
```

```{r lasso-coefs}
predict(rmodel, type="coefficient", s=kcvrmodel$lambda.1se, newx=X[1:10,])
```

Using a lambda value 1 standard deviation away from the lambda that minimizes RMSE, we see the LASSO regression has eliminated the Radius and Absolute Magnitude predictors. Seeing at least one of these removed was expected because these two predictors were highly correlated after transforming Radius. Although the LASSO regression eliminated the Yellow-White color, we opted to leave this categorical predictor in the model since the other 3 colors were found to be significant.

```{r good-model}
goodmodel <- lm(Temperature~L+Color, data=stars2)
summary(goodmodel)
```

Following feature selection using LASSO regression, we created our final linear model using the chosen features. All of the predictors are shown to be significant in the model with p-values < .01. 

```{r}
predictions <- exp(predict(goodmodel, stars2))
print(paste('RMSE:',sqrt(mean((predictions - stars$Temperature)^2))))
```

The Adj. $R^2$ and RMSE are about the same as the model pre-feature selection, but based on the principal of parsimony, we conclude that this model is better.

```{r jitter-plot}
ggplot(stars2, aes(x=L, y = Temperature, color=Color))+geom_jitter()+
  geom_smooth(method="lm", model.extract(stars2), se=FALSE)
```

#### Checking Assumptions

```{r good-model-resid-plot}
star_pred <- mutate(stars2, predictions=fitted(goodmodel), resid=residuals(goodmodel))

ggplot(star_pred, aes(x=predictions, y=resid, color=Color)) + geom_point() + geom_hline(yintercept = 0, color="red")
```

Generally speaking, the residuals plot is evenly distributed around the $y=0$ line. However, we see distinct grouping in the plot along the $y=0$ line. This comes from inherent grouping in the data based on the color of the star. This calls into question the independence assumption for linear modeling. Color and temperature do not appear to be independent. Some of this may have come from cleaning the data-set as we compressed the number of colors 9 from to 4. This came as a result of many colors being under-represented in the data-set.

```{r good-model-qq-plot}
ggplot(star_pred, aes(sample=resid)) + stat_qq() + stat_qq_line(color="red")
```

The QQ plot displays excellent results within the $[-1, 1]$ quantile range. However, after that we see the points diverge heavily from the $y=x$ line. This is an indication that the population may not be normally distributed in the extreme ranges of the data. This divergence calls the 4th assumption of a normal population into question.

#### Conculusion

Although we have a model that has relatively strong performance metrics, we struggle to validate all of the assumptions necessary for linear modeling. In the end, a linear model is likely not the best choice for our data-set. 

## Logistic Regression
